{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand your NLP models\n",
    "\n",
    "This is a torturial on how to utilize Interpreter class to explain certain hidden layers in your NLP models. We provide the explanation by measuring the information of input words ${\\bf x}_1,...,{\\bf x}_n$ that is encoded in hidden state ${\\bf s} = \\Phi({\\bf x})$. The method is from paper [*Towards a Deep and Unified Understanding of Deep Neural Models in NLP*](https://www.microsoft.com/en-us/research/publication/towards-a-deep-and-unified-understanding-of-deep-neural-models-in-nlp/) that is accepted by **ICML 2019**. In this torturial, we provide a simple example for you to start quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "\n",
    "# Suppose your input is x, and the sentence is simply \"1 2 3 4 5\"\n",
    "x = torch.randn(5, 256) / 100\n",
    "x = x.to(device)\n",
    "words = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "# Suppose your hidden state s = Phi(x), where\n",
    "# Phi = 10 * word[0] + 20 * word[1] + 5 * word[2] - 20 * word[3] - 10 * word[4]\n",
    "def Phi(x):\n",
    "    W = torch.tensor([10.0, 20.0, 5.0, -20.0, -10.0]).to(device)\n",
    "    return W @ x\n",
    "\n",
    "\n",
    "# Suppose this is your dataset used for training your models\n",
    "dataset = [torch.randn(5, 256) / 100 for _ in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Interpreter instance\n",
    "\n",
    "To explain a certain $\\bf x$ and certain $\\Phi$, you need to create an Interpreter instance, and pass your $\\bf x$, $\\Phi$ and regularization term (which is the standard variance of the hidden state r.v. $\\bf S$) to it. We also provide a simple function to calculate the regularization term that is needed in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interpreter()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Interpreter import calculate_regularization, Interpreter\n",
    "\n",
    "# calculate the regularization term.\n",
    "regularization = calculate_regularization(dataset, Phi, device=device)\n",
    "\n",
    "# create the interpreter instance\n",
    "# we recommend you to set hyper-parameter *scale* to 10 * Std[word_embedding_weight], 10 * 0.1 in this example\n",
    "interpreter = Interpreter(\n",
    "    x=x, Phi=Phi, regularization=regularization, scale=10 * 0.1, words=words\n",
    ")\n",
    "interpreter.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Interpreter\n",
    "\n",
    "Then, we need to train our interpreter to let it find the information loss in each input word ${\\bf x}_i$ when they reach hidden state $\\bf s$. You can control the iteration and learning rate when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 924.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the interpreter by optimizing the loss\n",
    "interpreter.optimize(iteration=5000, lr=0.5, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show and visualize results\n",
    "\n",
    "After training, we can show the sigma (directly speaking, it is the range that every word can change without changing $\\bf s$ too much) we have got. Sigma somewhat stands for the information loss of word ${\\bf x}_i$ when it reaches $\\bf s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00315634, 0.00181308, 0.00633237, 0.00174878, 0.0030807 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sigma we get\n",
    "interpreter.get_sigma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAABzCAYAAADNPJaYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABQBJREFUeJzt2z3InXcZx/HfpYZa0ki0VglG7KRLQSuhS8BBi/hSxLGDToUuHSoOgovg4lh0Lb6A+LZUFwVfoBWpqDVJU61Gtwqi8qgl1OBg0cshRychicfmf93HzwcenvPAPfz4L99z7vs81d0BgGletnoAAPwnAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEivuJGLj5042bfcfuql2vJ/4a/Pv7B6wqbd/ebXrZ6weU//+mj1hM279TUnVk/YtL/9+Q958crlutZ1NxSoW24/lbd+4gv//Spy4SuPr56waT/8zkOrJ2zeq+/9zOoJm3fX/e9aPWHTnv3UA9d1nVt8AIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMdM1AVdWDVXWuqs69eOXyzdgEANcOVHc/2t1nuvvMsdtO3oxNAOAWHwAzCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIwkUACNVd1//xVV/TPKbl27O3l6b5E+rR2ycM9yP89ufM9zf9DN8U3ffca2LbihQ01XVue4+s3rHljnD/Ti//TnD/R3KGbrFB8BIAgXASIcWqEdXDzgAznA/zm9/znB/B3GGB/UMCoDDcWifoAA4EAcRqKr6fFUdVdWzq7dsUVW9saqeqKpLVfWLqnp49aatqapXVtVTVfXM7gw/uXrTFlXVy6vq6ar65uotW1RVz1XVz6vqYlWdW71nXwdxi6+q3pHkSpIvdvddq/dsTVWdSnKquy9U1Ykk55N8sLt/uXjaZlRVJTne3Veq6liSJ5M83N0/XjxtU6rqo0nOJHlVd9+3es/WVNVzSc509+T/gbpuB/EJqrt/kOT51Tu2qrt/390Xdq//kuRSkjesXbUtfdWV3Z/Hdj/bf/d3E1XV6STvT/LZ1VuY4SACxf9OVd2Z5O4kP1m7ZHt2t6cuJjlK8r3udoY35tNJPpbkH6uHbFgn+W5Vna+qB1eP2ZdA8W9VdVuSx5J8pLtfWL1na7r77939tiSnk9xTVW43X6equi/JUXefX71l485299uTvDfJQ7vHH5slUCRJds9NHkvy5e7++uo9W9bdl5N8P8l7Fk/ZkrNJPrB7hvK1JO+sqi+tnbQ93f273e+jJN9Ics/aRfsRKP71gP9zSS519yOr92xRVd1RVSd3r29Ncm+SX61dtR3d/fHuPt3ddya5P8nj3f2hxbM2paqO777klKo6nuTdSTb9zeaDCFRVfTXJj5K8pap+W1UPrN60MWeTfDhX37Ve3P28b/WojTmV5Imq+lmSn+bqMyhfleZmen2SJ6vqmSRPJflWd3978aa9HMTXzAE4PAfxCQqAwyNQAIwkUACMJFAAjCRQAIwkUACMJFAAjCRQAIz0TzZ3AtGXLyIjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the information loss of our sigma\n",
    "interpreter.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the second and forth words are important to ${\\bf s} = \\Phi({\\bf x})$, which is reasonable because the weights of them are larger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
